Open in app
Sidebar menu
Search
Write
Notifications

your-mommas-favorite
From Prompt Engineering to Knowledge Graphs — Part 1
Evaluating LLM Prompts for Entity & Relation Extraction, and Why Complexity Never Disappears
Fanghua (Joshua) Yu
Fanghua (Joshua) Yu

Follow
12 min read
·
Dec 16, 2025
160


5





Press enter or click to view image in full size

Image generated by Gemini-3 by author
1. Introduction
LLM-based knowledge extraction looks deceptively simple by sending a prompt like this to an LLM: “extract entities and relations from this text.” Prompting methods are basically different ways to control attention, decomposition, and grounding — without changing model weights. Some reduce variance (stability), some add reasoning structure, and some introduce external evidence (retrieval/tools).

However, in practice, teams usually hit one (or more) of five bottlenecks:

Format compliance: outputs aren’t parseable (schema drift, extra prose, inconsistent IDs)
Recall: misses entities/relations that are clearly present
Precision: extracts lots of facts… but many are wrong or not supported by the text
Domain grounding: wrong ontology mapping / normalization (IDs, labels, jargon)
Cost/latency: great quality in the lab, unusable at scale
That’s why there isn’t one “best prompting method.” Different methods optimize different failure modes — and often trade one metric for another.

This post gives you 12 practical prompting methods for extraction, then shows how they perform with different LLMs.

2. The 12 Prompting Methods
1) Zero-shot
Definition: Task + strict output schema, no examples.
Feature: Minimum viable extraction baseline.
Why it works: Modern LLMs can follow precise structural constraints surprisingly well when instructions are tight.
When it doesn’t: Long/ambiguous text, heavy jargon, inconsistent formatting under variation.

2) One-shot
Definition: Provide exactly one example (input → output).
Feature: A single “anchor” that teaches your schema and granularity.
Why it works: In-context learning — models adapt behavior from examples.
When it doesn’t: Overfits the example; fails when the new doc differs in style or ontology.

3) Few-shot
Definition: Multiple curated examples that cover typical and edge cases.
Feature: A mini training set inside the prompt.
Why it works: More coverage improves consistency across formats and tricky patterns.
When it doesn’t: Prompt bloat (cost/latency), conflicting examples, increased “format drift” when context gets large.

4) CoT (Chain-of-Thought)
Definition: Ask the model to reason step-by-step before final extraction.
Feature: Makes intermediate reasoning explicit.
Why it works: Helps when relations are implied (cross-sentence logic, “therefore” facts).
When it doesn’t: If extraction is mostly span-finding; can produce plausible but incorrect reasoning → confident errors.

Generate a knowledge graph as complete as possible for the text below. 
The output should be a list of unique nodes and edges suitable for knowledge graph construction in KGQA and Graph RAG. 
Nodes typically represent entities (stages, organisms, objects, concepts) and edges represent relations (transformations, interactions, attributes, locations).

Think through the following steps internally (do NOT write them out):
1. Identify all important entities in the text.
2. Group them into unique nodes (name + label).
3. Identify all meaningful relations between the nodes.
4. Map them into edges using exact node names.

... ... ... ...
5) Decomposition (least-to-most / staged extraction)
Definition: Split extraction into stages (entities → candidate relations → normalize → final triples).
Feature: “Easy-to-hard” breakdown reduces cognitive load.
Why it works: Each step is simpler; failures become debuggable (“entity stage failed” vs “everything failed”).
When it doesn’t: More calls/cost; early misses propagate unless you add “second pass” recovery.

Generate a knowledge graph as complete as possible for the text below. 
The output should be a list of unique nodes and edges suitable for knowledge graph construction in KGQA and Graph RAG. 
Nodes typically represent entities (stages, organisms, objects, concepts) and edges represent relations (transformations, interactions, attributes, locations).

Decomposition steps (for your internal reasoning):
- Step 1: Identify all candidate entities (stages, organisms, objects, concepts).
- Step 2: Identify all semantic relations (transformations, interactions, attributes, locations).
- Step 3: Deduplicate entities and construct nodes.
- Step 4: Construct edges using exact node names.
6) Self-Ask
Definition: The model asks itself follow-up questions before final output.
Feature: Structured sub-questions expose missing info needs.
Why it works: Great for multi-hop documents where linking depends on answering intermediate questions.
When it doesn’t: Sub-questions drift; if the text lacks evidence, the model may invent answers unless you enforce abstention/evidence.

Generate a knowledge graph as complete as possible for the text below. 
The output should be a list of unique nodes and edges suitable for knowledge graph construction in KGQA and Graph RAG. 
Nodes typically represent entities (stages, organisms, objects, concepts) and edges represent relations (transformations, interactions, attributes, locations).

Internally, you should ask yourself questions such as:
- What are the key entities in this text?
- What lifecycle events or transformations occur?
- How should they be connected as relations?

However, do NOT write these questions or reasoning out.
7) Role prompting
Definition: Give an identity like “You are a strict information extraction engine…”
Feature: Frames style and risk tolerance (e.g., “only extract explicitly stated facts”).
Why it works: Can improve adherence to constraints in some settings (e.g., “expert-style” prompting).
When it doesn’t: Personas often don’t improve factual accuracy by themselves and effects can be inconsistent.


You are a senior ontologist specializing in biological knowledge graphs for Question Answering (KGQA) and Graph Retrieval-Augmented Generation (Graph RAG).

Generate a knowledge graph as complete as possible for the text below. The output should be a list of unique nodes and edges suitable for knowledge graph construction in KGQA and Graph RAG. Nodes typically represent entities (stages, organisms, objects, concepts) and edges represent relations (transformations, interactions, attributes, locations).

Use concise but informative labels for each node (e.g., organism, stage, plant_part, behavior, location).
Use relation names that are suitable for real-world KG schemas (e.g., hatches_into, transforms_into, feeds_on, lays_on, located_in).
8) Auto-fact (atomic fact extraction)
Definition: First extract atomic, verifiable facts, then convert them into triples/graph updates.
Feature: Separates “what is claimed” from “how we structure it,” enabling better verification and provenance. (Atomic-fact framing is used in factuality evaluation work.)
Why it works: Atomic facts are easier to validate against evidence spans; reduces “one big wrong triple” errors.
When it doesn’t: Over-fragmentation creates noise; paraphrasing can still introduce unsupported claims unless you require evidence spans.

You are an expert extractor that converts natural-language text into atomic semantic facts.
Your goal is to extract MEANING, not surface wording.

Each fact must follow the rules of:
- canonicalization
- event detection
- verb normalization
- modifier / qualifier extraction
- negation handling
- semantic role assignment
9) GenKnow (Generated Knowledge Prompting)
Definition: Generate relevant background knowledge first, then extract.
Feature: Uses the LLM as a “knowledge scratchpad” to prime relation discovery.
Why it works: Helps when relations require domain context or commonsense bridging.
When it doesn’t: High hallucination risk — background knowledge can be wrong/outdated; you must add evidence gating if correctness matters.

Your task is extracting knowledge triples from text. 
A knowledge triple consists of three elements: subject - predicate - object. 
Subjects and objects are entities and the predicate is the relation between them. 
Before extracting triples, generate knowledge about the entities in the text and potential relations between them. 
Here is an example:
Text: Leaves absorb sunlight. 
Knowledge: Leaves: plant_part; Sunlight: energy 
The following triples can be extracted considering the knowledge. 
Triples: (Leaves, absorb, Sunlight) 
Example ends here. 
Generate knowledge as shown in the example and extract knowledge triples from the input passage. 
Reason freely, but only output the final nodes and edges using this format:
10) ReAct (Reason then Act)
Definition: Interleave reasoning with actions (tool calls), using observations to proceed.
Feature: Adds an iterative loop: resolve entities, look up schema/KB, then finalize.
Why it works: Grounding becomes explicit — great for entity resolution, ontology lookup, and recovery from uncertainty.
When it doesn’t: Tooling adds complexity; bad tools/retrieval can make outputs worse than a simpler prompt.

Your task is extracting knowledge triples from text. 
A knowledge triple consists of three elements: subject - predicate - object. 
Subjects and objects are entities and the predicate is the relation between them. 
Let's use an example:
Text: Leaves absorb sunlight. 
Thought 1: I need to determine the entities. 
Act 1: Named entity extraction. 
Observation 1: Leaves; Sunlight 
Thought 2: What type of entities do I have? 
Act 2: Named entity tagging 
Observation 2: Leaves: plant_part; Sunlight: energy 
Thought 3: What are the potential relations between these entities? 
Act 3: List the potential relations 
Observation3: Leaves absorb Sunlight 
Thought 4: What are the triples? 
Act 4: Form the triples 
Observation4: (Leaves, absorb, Sunlight) 
Thought5: I have extracted knowledge triples from the input text. 
Act5 : Finish 
Observation5: Task is completed. 

Before answering a query, think and decide your act as shown above. 
Extract the knowledge triples from the following text, but only output the final nodes and edges in this format:
11) RAG (retrieval-augmented generation)
Definition: Retrieve relevant context (ontology, glossary, entity registry, past examples) and inject it into the prompt.
Feature: Combines parametric knowledge with non-parametric memory to improve grounding and updatability.
Why it works: Improves normalization and relation labeling; reduces “schema guessing” when your domain is specialized.
When it doesn’t: Retrieval misses or conflicts; prompt injection risks if retrieved sources aren’t trusted.

Extract knowledge triples from the text. Here are a few examples:
Text: Birds lay eggs in nests. 
Triples: (Birds, lay, Eggs); (Eggs, located_in, Nests) 
Examples end here. 
Apply the same reasoning to the input and output only the nodes and edges using this format:
12) Self-consistency
Definition: Sample multiple outputs and pick the most consistent answer (vote/aggregate).
Feature: Inference-time ensembling reduces variance from a single decode.
Why it works: Stabilizes brittle extractions; improves reasoning tasks when multiple valid chains lead to one answer.
When it doesn’t: Cost multiplies; if the model is consistently biased, it will consistently be wrong.


Your task is extracting knowledge triples from text. 
A knowledge triple consists of three elements: subject - predicate - object. 
Subjects and objects are entities and the predicate is the relation between them. 
Let's use a few examples:
Text: A caterpillar turns into a butterfly. 
Let's extract the entities first. 
Here is the list of the entities in this text: Caterpillar; Butterfly What do you know about the entities? 
Caterpillar: organism; Butterfly: organism 

Now we think about the potential relations between these entities: Caterpillar transforms into Butterfly 

Let's make a draft of the triples. 
(Caterpillar, transforms_into, Butterfly); (Butterfly, transforms_into, Caterpillar) 

Now it is time to think and filter out incorrect triples if there is any. 
The following triples seem to be incorrect: 
(Butterfly, transforms_into, Caterpillar). 

Here is the reason why I think these triples are incorrect: Butterflies do not transform back into caterpillars. Therefore, final triples should be: (Caterpillar, transforms_into, Butterfly).
Think like a domain expert and check the validity of the triples. Keep track of your thinking as shown in the example and extract triples from the following text. Only output the final nodes and edges in this format:
3. PromptFactory — Test & Comparison
I compiled all 12 prompting methods into a factory class and shared code on github, together with testing reports with different LLMs.

GitHub - Joshua-Yu/prompt-engineering-for-kg: Test various prompt engineering methods for knowledge…
Test various prompt engineering methods for knowledge extraction tasks by LLM - Joshua-Yu/prompt-engineering-for-kg
github.com

prompt_factory.py centralizes all prompt engineering variants used by the KG extractor. Each method (zero-shot, few-shot, CoT, etc.) returns the same nodes/edges output format so downstream parsers can remain consistent.

Basic Usage
from kgextractor.prompt_factory import KGPromptFactory

factory = KGPromptFactory()
prompt_text = factory.build_prompt("few_shot", sample_text)
build_prompt accepts any method named in KGPromptFactory.methods. To preview every prompt for a passage run the module directly:

python kgextractor/prompt_factory.py
Set PROMPT_FACTORY_RUN_LLM=1 to actually call the configured models; otherwise it only prints the prompts.

Environment variables
PROMPT_FACTORY_MODEL: generation model (defaults to gpt-5-mini when running the script, gpt-4o-mini inside helper calls).
PROMPT_FACTORY_EVAL_MODEL: overrides the evaluator model (falls back to PROMPT_FACTORY_MODEL).
PROMPT_FACTORY_TEMPERATURE: optional float forwarded to OpenAI. Leave unset for models that enforce their built-in temperature (e.g., GPT‑5).
PROMPT_FACTORY_RUN_LLM: set to 1 to enable live LLM calls inside main().
Evaluation Loop
When PROMPT_FACTORY_RUN_LLM=1, the script:

Builds each prompt for a sample passage.
Calls the generation model via run_prompt_via_openai.
Parses nodes/edges and re-sends them to the evaluator prompt (evaluate_graph_with_llm).
Stores prompt, generation, and feedback under results/<model>/prompt_test_<method>_<model>.txt.
Results
GPT-4o / temperature=0.1

Press enter or click to view image in full size

Table 1. Prompting Methods Evaluation with GPT-4o
GPT-5-mini

Press enter or click to view image in full size

Table 2. Prompting Methods Evaluation with GPT-5-mini
I need to highlight that the results shown here are NOT meant to be used as a benchmark or guidance on what prompting methods are better than others, as there was only one paragraph used for testing. In production there may be many variances stemming from differences in evaluation setup, retrieval quality, evidence grounding, prompt/task design, and what the metric is actually measuring.

The code is more of a framework for evaluating prompting methods for the task.

In fact, some published evaluations do find RAG (Retrieval-Augmented Generation) performing best and CoT (Chain-of-Thought) providing modest or no gain for extraction-like tasks. As seen in test logs above:

RAG was middling (mixed/partial, overall ~0.7–0.72 range) as in my tests the RAG sampling was fixed.
CoT scored low with both GPT-4o and GPT-5-mini, which is aligned with researches.
Some simpler methods like few-shot / one-shot were improved a lot from GPT-4o to GPT-5-mini which may indicate the latter has lifted relevant skills in this release
GenKnow first expands context (“generate knowledge about entities and relations”) before extraction. At low temperature at 0.1, it sticks to probable, text-aligned completions, so the generated background reinforces rather than distorts the source.
Self-Ask performs better when its internal “self-questions” are predictable. At low temperature, it consistently asks the most obvious ones (“What binds to receptors?”, “What sends signals to the brain?”).
Some degradations / variations from results of GPT-5-mini would require further investigation.

Conclusion
Real-world projects rarely behave like benchmark papers — especially when building knowledge extraction, Graph RAG, or AI-powered analytics pipelines. From hundreds of test runs across prompting styles and models, a few lessons stand out:

No single method wins everywhere.
Few-shot and Role prompts stabilize format; Self-Ask and GenKnow excel when reasoning or context bridging is needed; RAG helps only if retrieval is clean and domain-aligned.
Schema beats style.
The best prompts enforce strict output schemas (e.g., JSON or “nodes/edges” format) with clear “if unsure, output null” rules. Structure consistency matters more than clever wording.
Retrieval quality > Retrieval quantity.
RAG fails if retrieval adds noise. Curate ontology and glossary sources carefully — a small, precise index usually outperforms large, generic ones.
Low randomness improves precision.
For extraction, deterministic decoding (low temperature or grammar-constrained generation) yields cleaner, repeatable outputs. When temperature control isn’t available (e.g., GPT-5), use schema locking, top_p≈0.2, and self-verification instead.
Hybrid orchestration wins in production.
Combine passes:
“Cold” deterministic extraction for precision
“Warm” exploratory generation for recall
Automated merging + validation layer to unify results
6. Evaluate with your own metrics.
Academic F1 scores hide real issues like duplication, schema drift, and missing evidence links. Adding weasures like completeness, correctness, and confidence against your task definition.

7. Start simple, then specialize.
Zero-shot with a clear schema gives a reliable baseline. Add complexity (RAG, CoT, decomposition) only when your data truly demands it.

8. LLMs are different — learn their “personality.”
Each model family (GPT-4o, GPT-5, Claude, Gemini, Qwen, etc.) has distinct decoding biases and reasoning habits.

Behavioral calibration: Run a small battery of structured tasks (e.g., schema-following, relation chaining, summarization) to gauge how each model handles precision vs abstraction.
Prompt familiarization: Keep a shared “prompt tuning log” — note which phrasings yield consistent behavior per model.
Cross-model validation: Use a second model as an auditor or verifier to detect omissions or hallucinations.
Familiarity breeds reliability: treat models like new team members who must be onboarded and evaluated before production.
My final words,

Complexity Doesn’t Go Away — It Just Moves.

Every optimization — prompt design, retrieval, or schema enforcement — changes where you handle complexity, not whether it exists. The best AI systems make that complexity visible, structured, and manageable.

References
Wei, J. et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. (2022). arXiv
Wang, X. et al. Self-Consistency Improves Chain of Thought Reasoning in Language Models. (2022). arXiv
Zhou, D. et al. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. (2022). arXiv
Press, O. et al. Measuring and Narrowing the Compositionality Gap in Language Models (introduces Self-Ask). (2022). arXiv
Yao, S. et al. ReAct: Synergizing Reasoning and Acting in Language Models. (2022/2023). arXiv
Lewis, P. et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. (2020). arXiv
Liu, J. et al. Generated Knowledge Prompting for Commonsense Reasoning. (ACL 2022). ACL Anthology
Min, S. et al. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. (2023). arXiv
Xu, B. et al. ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. (2023). arXiv
Zheng, M. et al. Personas in System Prompts Do Not Improve Performance… (Findings of EMNLP 2024). ACL Anthology
Zhang, Z. et al. A Survey of Generative Information Extraction. (COLING 2025). ACL Anthology
Jiao, Y. et al. Instruction Tuning for On-Demand Information Extraction (InstructIE / ODIE). (EMNLP 2023). ACL Anthology
Genai
Prompt Engineering
Knowledge Graph
Evaluation
Rags
160


5




Fanghua (Joshua) Yu
Written by Fanghua (Joshua) Yu
1.3K followers
·
101 following
I believe our lives become more meaningful when we are connected, so is data. Happy to connect and share: https://www.linkedin.com/in/joshuayu/


Follow
Responses (5)
your-mommas-favorite
your-mommas-favorite
﻿

Cancel
Respond
Scott M
Scott M

Dec 21, 2025


Excellent piece, thank you. What do you think about Graphiti as a graph building (entity and relations extractor) utility?
--

Reply

Tim Dentry
Tim Dentry

Dec 28, 2025


This is a great article. What I really appreciated was that you included test and results data in your post.
--

Reply

Arix Fïen
Arix Fïen

he
Dec 21, 2025


Sharp and very grounded piece.
What stood out most is the honest framing that prompting methods don’t eliminate complexity, they relocate it. The way you map each technique to specific failure modes (format drift, recall, grounding, cost) is far more…more
--

Reply

See all responses
More from Fanghua (Joshua) Yu
How Do We Know If The Knowledge Graph Is Any Good?
Fanghua (Joshua) Yu
Fanghua (Joshua) Yu

How Do We Know If The Knowledge Graph Is Any Good?
Evaluating LLM-based Knowledge Graph Extraction
Dec 1, 2025

What Really Matters to Better GraphRAG Implementation? — Part 1
Fanghua (Joshua) Yu
Fanghua (Joshua) Yu

What Really Matters to Better GraphRAG Implementation? — Part 1
A Data-Driven Analysis of Graph Types, Operators, Costs, and Retrieval Performance
Dec 9, 2025

Graph RAG Is Cool, But Which Graph?
Fanghua (Joshua) Yu
Fanghua (Joshua) Yu

Graph RAG Is Cool, But Which Graph?
An Overview on Graph Types for Graph RAG: Structures, Schemas, and When to Use Each One
Nov 20, 2025

From Toy Demo to Scalable Graph RAG: Rethink Your Indexing
Fanghua (Joshua) Yu
Fanghua (Joshua) Yu

From Toy Demo to Scalable Graph RAG: Rethink Your Indexing
A deep dive into the indexing strategies
Nov 26, 2025

See all from Fanghua (Joshua) Yu
Recommended from Medium
Building Your First Ontology: A Hands-On Tutorial
Pankaj Kumar
Pankaj Kumar

Building Your First Ontology: A Hands-On Tutorial
Stop Reading About Ontology. Start Building One.

Dec 13, 2025

Ontology-Driven GraphRAG: A Framework for Zero-Noise Knowledge Extraction
Akash Goyal
Akash Goyal

Ontology-Driven GraphRAG: A Framework for Zero-Noise Knowledge Extraction
Building a self-improving knowledge graph that doesn’t just store data, it understands, validates, and evolves it

Dec 4, 2025

Google Finally Solved The Fragile Text-to-SQL Systems
Coding Nexus
In

Coding Nexus

by

Minervee

Google Finally Solved The Fragile Text-to-SQL Systems
Stop Shipping Demo-Grade Text-to-SQL Systems and Do The Last Replacement Right Now

Nov 6, 2025

Building Agentic RAG on Neo4j’s Knowledge Graph
Towards AI
In

Towards AI

by

Yogender Pal

Building Agentic RAG on Neo4j’s Knowledge Graph
Vector embeddings retrieve “similar text.” Neo4j retrieves the truth. Here’s how Agentic RAG brings them together.

Dec 9, 2025

How to Read One Book Per Week (Even if You Read Slowly)
Scott H. Young
Scott H. Young

How to Read One Book Per Week (Even if You Read Slowly)
Become the person who can easily ready 50+ books in a year

Dec 10, 2025

I Thought I Knew System Design Until I Met a Google L7 Interviewer
Beyond Localhost
In

Beyond Localhost

by

The Speedcraft Lab

I Thought I Knew System Design Until I Met a Google L7 Interviewer
A single whiteboard question revealed the gap between knowing patterns and actually designing systems that scale.

Dec 22, 2025

See more recommendations