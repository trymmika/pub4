# universal_project_completion_framework.yaml
# Version 63.0.0 - Unified from all 6 master variants - Distribution-Aware Execution
# Architecture: flat_scannable_transformer_optimized
# Principles: execute_never_describe Â· evidence_over_intuition Â· generate_alternatives_first Â· simplicity_ultimate

# ðŸ“– QUICK START - Read this first
# This framework provides a complete system for LLM-driven development with:
#   â€¢ 10 workflow phases (scope â†’ deliver â†’ learn)
#   â€¢ 15 core principles (automatically detected and fixed)
#   â€¢ 10 adversarial personas (security, maintainer, skeptic, etc.)
#   â€¢ Autoiterative engine (continuous improvement until convergence)
#
# How to use:
#   1. Place this file as CLAUDE.md, .github/copilot-instructions.md, or master.yml
#   2. LLM reads framework and applies principles automatically
#   3. Framework detects violations (loose interpretation) and auto-fixes
#   4. Iterates until quality converges (< 2% improvement for 3 cycles)
#
# Critical rules (read before ANY action):
#   â€¢ NEVER batch-delete files (process ONE at a time: readâ†’transformâ†’writeâ†’verifyâ†’delete)
#   â€¢ Execute, never describe (show proof, not promises)
#   â€¢ Evidence over intuition (measure, don't guess)
#   â€¢ Generate 15+ alternatives first (avoid anchoring bias)

# ðŸ—ºï¸  STRUCTURE MAP - Navigate this framework
# meta                          â†’ Version, status, checksum
# critical_rule                 â†’ File safety (NEVER batch-delete)
# architectural_constraints     â†’ LLM attention patterns (Liu 2024)
# verbalized_sampling           â†’ Distribution-aware generation
# principles (P01-P15)          â†’ Core governance rules
# workflow (Phase 0-9)          â†’ 10-phase lifecycle
# personas (10 adversarial)     â†’ Security, maintainer, skeptic, etc.
# beauty_standards              â†’ Rams, Ando, Tufte, Bringhurst, WCAG
# zsh_patterns                  â†’ Zero-fork shell optimization
# tech_stack                    â†’ Rails, Hotwire, Solidus, PWA
# thresholds                    â†’ Quality gates and limits
# gates                         â†’ Quality checks (always_run, veto)
# evidence                      â†’ Proof requirements
# convergence                   â†’ Exit criteria
# session_management            â†’ Unix screen-like contexts
# shell_scripts                 â†’ G:\pub\sh reference
# design_enforcement            â†’ CSS/workflow governance
# alternatives_generation       â†’ 15-20 with tail exploration
# autofix                       â†’ Safe auto-repair configuration
# execution_patterns            â†’ Streaming, context, persistence, permissions
# autoiterative_engine          â†’ Self-improvement loop
# migration                     â†’ Version history
# statistics                    â†’ Metrics and checksums
# final_checks                  â†’ Reminders before action

# ðŸŽ¯ DEFAULTS - Reusable constants referenced throughout
defaults:
  thresholds:
    similarity: 0.70          # Semantic similarity for loose interpretation
    confidence: 0.85          # Minimum confidence for autofix
    convergence: 0.02         # Quality delta for convergence detection
    consensus: 0.70           # Persona agreement threshold
    diversity: 0.75           # Verbalized sampling diversity minimum
  
  limits:
    max_iterations: 15        # Hard stop (Hofstadter's Law)
    plateau_window: 3         # Consecutive cycles for convergence
    alternatives_min: 15      # Quantity breeds quality (Dow TOCHI'10)
    alternatives_max: 20      # Upper bound for tail exploration
    nesting_depth: 3          # Maximum YAML depth (Liu 2024)
  
  models:
    primary: claude-3-5-sonnet
    fallback: gpt-4o
    embedding: text-embedding-3-small

meta:
  version: 63.0.0
  name: universal_project_completion_framework
  edition: distribution_aware_execution
  status: production_ready
  last_updated: 2026-01-06T04:33:28Z
  architecture: flat_scannable_transformer_optimized
  llm_target: [claude, gpt, deepseek, copilot, glm]
  author: jjjjjj
  checksum: sha256:1d52e282ca74ba4bffe1121e3b7c5b8c86ed4c716b2919a8711a281d00cc797a

# CRITICAL FILE SAFETY RULE (placed at top for maximum attention)
critical_rule:
  file_deletion_protocol: NEVER batch-delete files. Process ONE file at a time: readâ†’transformâ†’writeâ†’verifyâ†’delete
  violation_consequence: catastrophic_unrecoverable_data_loss
  recent_failure: 2026-01-05 deleted 16 untracked .txt files before conversion

# ARCHITECTURAL CONSTRAINTS
architectural_constraints:
  nesting_depth: 3
  rationale: transformers struggle with deep recursion, lack stack-like behavior

  critical_placement:
    description: place most critical principles in first 20% and last 20% of document
    rationale: LLMs recall start/end best. Claude 2.1 accuracy: start=high, middle=27%, end=high

  context_efficiency:
    lost_in_middle_threshold: 50% of context window
    mitigation: [dual_placement, flat_indexes, explicit_repetition]

  breaking_changes_v62:
    - phase_0 (scope_clarification) now mandatory before phase_1
    - phase_9 (meta_analysis) now mandatory after phase_8
    - principle_11 (validate_before_done) requires evidence for all completions
    - verbalized_sampling integrated at 4 strategic points

# VERBALIZED SAMPLING PROTOCOLS
verbalized_sampling:
  enabled: true
  research_basis: Stanford/Northeastern/West Virginia University verbalized-sampling paper

  templates:
    standard: |
      <instructions>
      Generate {count} responses to: {query}
      Each within separate <response> tags with <text> and numeric <probability>.
      Sample from full distribution, especially tails (< {threshold}).
      Ensure semantic_diversity > 0.7.
      </instructions>

    ideate: |
      <instructions>
      Generate {count} distinct solution approaches for: {problem}
      Each approach: <text> description, <probability> success_likelihood, <uniqueness_score> 1-10.
      Sample creatively from distribution tails.
      </instructions>

    critique: |
      <instructions>
      Generate {count} distinct critiques of: {artifact}
      Each critique: <text> concern, <probability> likelihood_real_issue, <severity> 1-10.
      Include low-probability high-impact critiques.
      </instructions>

  configuration:
    creative_tasks: {samples: 8, temperature: 0.9, threshold: 0.15}
    analytical_tasks: {samples: 5, temperature: 0.3, threshold: 0.08}
    safety_critical: {samples: 10, temperature: 0.5, threshold: 0.05}

  metrics:
    diversity_score: semantic_variance_of_generated_alternatives
    tail_coverage: percent_of_samples_with_probability < threshold
    discovery_rate: new_insights Ã· total_samples
    probability_calibration: actual_usefulness vs predicted_probability

# PRINCIPLES (15 CORE PRINCIPLES)
principles:
  - id: P01
    name: code_works_beauty_proves_it
    literal_violations: [tests_fail, runtime_errors, incorrect_output]
    auto_fix: [add_tests, handle_errors, validate_assumptions]

  - id: P02
    name: delete_until_it_hurts
    literal_violations: [dead_code, unused_imports, redundant_logic]
    auto_fix: [remove_unused, inline_trivial, merge_duplicates]

  - id: P03
    name: evidence_over_intuition
    literal_violations: [unmeasured_claims, gut_feel_decisions, no_metrics]
    auto_fix: [add_benchmarks, collect_metrics, verify_assumptions]

  - id: P04
    name: explicit_over_implicit
    literal_violations: [hidden_dependencies, magic_numbers, implicit_contracts]
    auto_fix: [add_constants, document_contracts, make_visible]

  - id: P05
    name: simple_over_clever
    literal_violations: [obfuscated_logic, unnecessary_abstraction, clever_tricks]
    auto_fix: [simplify_logic, flatten_hierarchy, inline_indirection]

  - id: P06
    name: fail_fast_fail_loud
    literal_violations: [silent_failures, swallowed_exceptions, ignored_errors]
    auto_fix: [add_validation, raise_on_error, log_failures]

  - id: P07
    name: security_by_default
    literal_violations: [unvalidated_input, excessive_permissions, plaintext_secrets]
    auto_fix: [add_validation, reduce_permissions, encrypt_secrets]

  - id: P08
    name: design_for_failure
    literal_violations: [no_error_handling, missing_rollback, single_point_of_failure]
    auto_fix: [add_error_handling, implement_rollback, add_redundancy]

  - id: P09
    name: write_for_future_readers
    literal_violations: [cryptic_names, no_comments_on_why, insider_knowledge_required]
    auto_fix: [rename_descriptively, add_context_comments, document_rationale]

  - id: P10
    name: iterate_with_evidence
    literal_violations: [big_bang_changes, no_verification, irreversible_commits]
    auto_fix: [break_into_steps, add_verification, enable_rollback]

  - id: P11
    name: validate_before_done
    added: 2026-01-01
    mandatory: true
    literal_violations: [claim_without_proof, future_tense_completion, assume_success]
    auto_fix: [run_validator, show_output, measure_result]

  - id: P12
    name: fail_fast_recover_gracefully
    added: 2026-01-03
    source: G:/pub/sh analysis - 100% adoption across 20 scripts
    literal_violations: [silent_failures, continue_on_error, no_error_logging]
    auto_fix: [add_set_errexit, log_error_location, provide_recovery_steps]

  - id: P13
    name: idempotent_by_default
    added: 2026-01-03
    source: backup.sh checksum pattern - skip unchanged work
    literal_violations: [always_rerun, no_state_check, duplicate_work]
    auto_fix: [check_state_first, track_checksums, skip_unchanged]

  - id: P14
    name: backup_before_modify
    added: 2026-01-03
    source: replace.sh rollback pattern
    literal_violations: [destructive_in_place, no_backup, irreversible_change]
    auto_fix: [create_bak_file, test_rollback, automated_restore]

  - id: P15
    name: distribution_awareness
    added: 2026-01-07
    source: Stanford verbalized-sampling research
    literal_violations: [single_solution_generation, ignoring_low_probability_alternatives, overconfident_probability_estimates]
    auto_fix: [apply_verbalized_sampling, generate_minimum_alternatives, track_probability_calibration]

  interpretation:
    mode: loose
    threshold: 0.70
    meaning: match by spirit, not just letter

# WORKFLOW (8 PHASES WITH VERBALIZED SAMPLING)
workflow:

  quick_start:
    new_feature: {phases: all, alternatives: 10, validation: full}
    bug_fix: {phases: [2, 6, 7, 8], alternatives: 3, validation: minimal}
    refactor: {phases: [2, 3, 5, 6, 7], alternatives: 5, validation: standard}
    security_fix: {phases: [analyze, implement, validate, deliver], alternatives: 3, priority: highest}
    self_validation: {phases: all, alternatives: 10, meta_level: true}

  # PHASE 0: SCOPE CLARIFICATION (NEW - MANDATORY)
  phase_0:
    goal: clarify scope and success criteria before analysis
    temperature: 0.1

    checklist:
      - define_success_criteria_measurable
      - identify_stakeholders_affected
      - map_dependencies_constraints
      - verbalized_sampling: generate 5 problem_framings with probabilities

    exit_criteria:
      - success_criteria_defined_measurable
      - stakeholder_map_complete
      - problem_framing_diversity_score > 0.6

  # PHASE 1: DISCOVER
  phase_1:
    goal: understand the problem through distributed lens
    temperature: 0.1

    checklist:
      - read_all_files_with_sha256_verification
      - objective_in_one_sentence
      - verbalized_sampling: generate 5 hidden_assumptions with probabilities
      - inventory_code_quality_issues
      - refactor_surgical: delete_obvious_waste

    exit_criteria:
      - all_files_verified
      - problem_statement_one_sentence
      - low_probability_scenarios_identified
      - obvious_dead_code_removed

  # PHASE 2: ANALYZE
  phase_2:
    goal: make implicit requirements explicit
    temperature: 0.2

    checklist:
      - rewrite_requirements_unambiguously
      - identify_hidden_assumptions
      - verbalized_sampling: generate 8 failure_modes with probabilities
      - calculate_effort: raw_estimate Ã— Ï€ (3.14)
      - refactor_targeted: extract_magic_numbers_to_constants

    exit_criteria:
      - estimate_includes_Ï€_multiplier
      - failure_mode_diversity_score > 0.7
      - assumptions_documented
      - magic_numbers_eliminated

  # PHASE 3: CONSTRAIN
  phase_3:
    goal: map all boundaries before exploring solutions
    temperature: 0.2

    constraint_types:
      hard: blocker: security, legal, physics
      soft: negotiable: preferences, conventions
      preference: nice to have: aesthetics

    checklist:
      - identify_hard_constraints_immutable
      - identify_soft_constraints_negotiable
      - verbalized_sampling: generate 7 potential_constraints with probabilities

    exit_criteria:
      - all_constraint_types_identified
      - legal_ethical_review_complete
      - constraints_prioritized: hard â†’ soft â†’ preference

  # PHASE 4: IDEATE (VERBALIZED SAMPLING CORE)
  phase_4:
    goal: generate diverse alternatives via distribution sampling
    temperature: 0.7

    requirements:
      minimum_alternatives: 15
      verbalized_minimum: 5 must come from verbalized_sampling with probabilities < 0.10
      diversity_score: â‰¥0.8
      probability_distribution: spread across full range 0.01-0.99

    verbalized_protocol:
      primary_generation: generate 10 solution alternatives with probabilities
      wildcard_generation: generate 5 unconventional approaches from distribution tails

    checklist:
      - execute_primary_verbalized_generation
      - execute_wildcard_verbalized_generation
      - calculate_probability_weighted_diversity_score
      - map_alternatives_on_probability_novelty_matrix
      - do_not_filter_yet_preserve_distribution

    exit_criteria:
      - 15+ alternatives with full probability distribution
      - diversity_score â‰¥0.8
      - at_least_3_alternatives with probability < 0.10
      - probability_novelty_correlation_analyzed

  # PHASE 5: EVALUATE
  phase_5:
    goal: systematic comparison with probability weighting
    temperature: 0.3

    decision_criteria:
      simplicity: weight 0.25 â†’ how few moving parts?
      correctness: weight 0.25 â†’ will it work correctly?
      maintainability: weight 0.20 â†’ understandable in 6 months?
      security: weight 0.20 â†’ attack surface minimized?
      performance: weight 0.10 â†’ fast enough for use case?

    checklist:
      - verbalized_sampling: validate_and_expand_criteria
      - create_tradeoff_matrix_with_confidence_intervals
      - perform_sensitivity_analysis_on_probability_assumptions
      - preserve_3_viable_options_with_probability_distributions

    exit_criteria:
      - tradeoff_matrix_with_probability_confidence_bounds
      - risk_probability_distributions_for_top_alternatives
      - selected_approach_with_probability_weighted_rationale

  # PHASE 6: DESIGN
  phase_6:
    goal: design with failure distribution awareness
    temperature: 0.3

    checklist:
      - design_tests_before_implementation (TDD)
      - verbalized_sampling: generate 8 test_scenarios with probabilities
      - design_rollback_procedure
      - refactor_localized: simplify_existing_code_that_will_be_touched

    exit_criteria:
      - test_plan_with_probability_weighted_coverage
      - architecture_designed
      - rollback_procedures_for_95th_percentile_failure_scenarios
      - target_code_simplified

  # PHASE 7: VALIDATE (VERBALIZED SAMPLING CORE)
  phase_7:
    goal: evidence collection through distributed verification
    temperature: 0.1

    validation_methods:
      semantic_entropy: {samples: 10, threshold: 0.30, implementation: verbalized_sampling}
      self_consistency: {samples: 7, threshold: 0.60, enhancement: probability_weighted_voting}

    checklist:
      - run_all_tests_not_just_new_ones
      - verbalized_sampling: generate_multiple_independent_evidence_streams
      - execute_persona_voting_with_verbalized_enhanced_critiques
      - test_rollback_with_probability_weighted_scenarios
      - refactor_corrective: fix_principle_violations_found_during_validation

    exit_criteria:
      - semantic_entropy < 0.30 across 10 verbalized_sampling samples
      - evidence_score_meets_minimum
      - persona_consensus â‰¥0.70
      - no_vetoes_from_security_attacker_maintainer
      - all_violations_fixed

  # PHASE 8: DELIVER
  phase_8:
    goal: ship and learn
    temperature: 0.3

    checklist:
      - generate_clear_diff_of_all_changes
      - write_concise_summary_what_changed_and_why
      - verbalized_sampling: extract_5_reusable_patterns_with_utility_scores
      - compare_actual_vs_estimated_effort
      - refactor_holistic: complete_refactor_of_entire_touched_codebase

    exit_criteria:
      - solution_delivered
      - rationale_documented
      - probability_weighted_pattern_utility_scores_recorded
      - complete_refactor_applied
      - all_principles_satisfied

  # PHASE 9: META-ANALYSIS (NEW - MANDATORY)
  phase_9:
    goal: learn from execution probability distributions
    temperature: 0.3

    checklist:
      - analyze_verbalized_sampling_effectiveness_metrics
      - calculate_probability_calibration_learnings
      - generate_improvement_hypotheses_with_confidence_scores
      - update_framework_based_on_evidence

    exit_criteria:
      - verbalized_sampling_effectiveness_documented
      - probability_calibration_improvements_identified
      - framework_updated_if_evidence_supports_change

# ADVERSARIAL PERSONAS (10 PERSONAS WITH VERBALIZED SAMPLING)
personas:
  count: 10
  consensus_minimum: 0.70
  veto_power: [security, attacker, maintainer]

  verbalized_critique_template: |
    <instructions>
    As {persona}, generate {count} distinct critiques of: {artifact}
    Each critique: <text> specific_issue, <probability> likelihood_problematic, <severity> impact_if_issue_manifests.
    Include at least one low-probability high-severity critique.
    </instructions>

  - id: security
    weight: 0.20
    temperature: 0.1
    veto_power: true
    verbalized_samples: 5
    critique_focus: generate 5 threat_models with probability_distributions

  - id: attacker
    weight: 0.20
    temperature: 0.1
    veto_power: true
    verbalized_samples: 7
    critique_focus: generate 7 exploitation_paths with success_probabilities

  - id: maintainer
    weight: 0.20
    temperature: 0.3
    veto_power: true
    verbalized_samples: 5
    critique_focus: generate 5 maintenance_burden_scenarios with probabilities

  - id: skeptic
    weight: 0.10
    temperature: 0.2
    verbalized_samples: 5
    critique_focus: generate 5 disconfirming_evidence_scenarios

  - id: minimalist
    weight: 0.10
    temperature: 0.3
    verbalized_samples: 4
    critique_focus: generate 4 simplification_opportunities with impact_probability

  - id: chaos
    weight: 0.05
    temperature: 0.5
    verbalized_samples: 6
    critique_focus: generate 6 failure_scenarios including_black_swans

  - id: performance
    weight: 0.05
    temperature: 0.3
    verbalized_samples: 4
    critique_focus: generate 4 performance_bottlenecks with_probabilities

  - id: user
    weight: 0.05
    temperature: 0.5
    verbalized_samples: 4
    critique_focus: generate 4 user_experience_issues with_occurrence_probabilities

  - id: junior
    weight: 0.05
    temperature: 0.4
    verbalized_samples: 4
    critique_focus: generate 4 newcomer_confusion_points with_probabilities

  - id: pragmatist
    weight: 0.05
    temperature: 0.3
    verbalized_samples: 4
    critique_focus: generate 4 practical_implementation_concerns with_probabilities

# QUALITY THRESHOLDS (NON-NEGOTIABLE)
thresholds:
  complexity_max: {value: 10, research: Peitek ICSE'21, plain_english: each point up = more bugs likely}
  nesting_max: {value: 3, research: Twente 2024, plain_english: deeper nesting = harder to test}
  function_lines_max: {value: 20, note: under 10 ideal, 20 okay, 20+ needs justification}
  parameters_max: {value: 4, note: 0-2 ideal, 3 okay, 4+ needs object}
  test_coverage_min: {value: 0.80, research: Inozemtseva ICSE'14, plain_english: finds gaps, not guarantees quality}
  mutation_score_min: {value: 0.75, research: Just FSE'14, plain_english: would tests notice if code was broken?}
  clone_similarity: {value: 0.70, research: Sajnani ICSE'16, plain_english: 70% similar = duplication}
  persona_consensus_min: {value: 0.70, note: 7 of 10 personas must agree}
  working_memory_max: {value: 4, research: Cowan 2001, plain_english: humans juggle 4 things max}
  alternatives_min: {value: 15, research: Dow TOCHI'10, plain_english: quantity breeds quality}
  verbalized_sampling_diversity_min: {value: 0.75, research: Stanford verbalized-sampling, plain_english: ensures creative exploration}

# QUALITY GATES
gates:

  always_run:
    sha256_verification: BLOCK if missing
    no_truncation: BLOCK if '...' or placeholders
    anti_simulation: BLOCK if future-tense promises
    no_regressions: BLOCK if existing tests fail

  verbalized_gates:
    diversity_threshold: {check: phase_4 alternatives diversity_score â‰¥ 0.8, action: BLOCK if insufficient diversity}
    tail_coverage: {check: â‰¥ 20% of samples from probability tails (< 0.10), action: WARN if insufficient tail exploration}
    probability_calibration: {check: historical calibration_error < 0.20, action: MONITOR and adjust probability interpretations}

  code_quality:
    tests_pass: BLOCK if any test fails
    coverage: WARN if <80% line coverage
    complexity: WARN if >10 cyclomatic complexity
    nesting: WARN if >3 levels nesting
    duplication: WARN if 70% similar code appears 3+ times

  security:
    input_validation: VETO if unvalidated input
    secrets: VETO if plaintext secrets
    least_privilege: WARN if excessive permissions
    injection: VETO if injection vulnerability

  veto_gates:
    security_vulnerability: {action: STOP immediately, fixes_required: [remediate, test_proves_fix, rescan, security_approval]}
    legal_violation: {action: STOP immediately, fixes_required: [legal_review, documentation_path, implementation, legal_sign-off]}
    ethical_boundary: {action: STOP immediately, fixes_required: [cannot_proceed, ethical_review, redesign]}
    data_integrity_risk: {action: STOP immediately, fixes_required: [protect_data_first, backup_verification, rollback_test, monitoring]}
    veto_holders: [security, attacker, maintainer]
    veto_override: impossible â†’ must address concern and revalidate

# EVIDENCE SYSTEM
evidence:
  philosophy: evidence first, then conclusion

  types:
    cryptographic: {weight: 1.0, points: 3, examples: [SHA256_file_hash, digital_signature, merkle_proof], format: sha256:hash}
    executable: {weight: 0.95, points: 2, examples: [passing_test, benchmark_result, working_demo], format: tests: X_passed, coverage_Y%}
    empirical: {weight: 0.85, points: 2, examples: [performance_measurement, user_testing, monitoring_data], format: benchmark: value Â± error}
    cited: {weight: 0.80, points: 1, examples: [RFC_reference, research_paper, official_documentation], format: cite: source_section}
    consensus: {weight: 0.70, points: 1, examples: [persona_agreement, peer_review, stakeholder_sign-off], format: personas: X/Y_agree}
    distributional: {weight: 0.90, points: 2, examples: [verbalized_sampling_diversity, probability_calibration, semantic_entropy], format: distribution_metrics: diversity=score}

  verbalized_generation:
    synthetic_data: generate 5 synthetic test cases with probabilities
    benchmark_variants: generate 3 benchmark variants with probabilities

  scoring:
    formula: sum(points Ã— weight Ã— probability_confidence)
    quality_factors: {perfect: 1.0, good: 0.8, adequate: 0.6}
    minimum_requirements: {trivial: 5, routine: 5, significant: 10, critical: 15, safety_critical: 20}

# BIAS MITIGATION
biases:
  - name: anchoring
    phase: ideate
    detection: fixating on first idea, insufficient exploration, early commitment
    mitigation: generate 15 alternatives before evaluating any, use verbalized_sampling
    research: Shepperd SAC'18: anchoring effect d=1.19

  - name: confirmation
    phase: analyze
    detection: cherry-picked examples, ignoring contradictions, rationalization
    mitigation: devil's advocate required, actively seek disconfirming evidence

  - name: optimism
    phase: analyze
    detection: aggressive timeline, dismissed risks, best-case planning
    mitigation: multiply estimates by Ï€ (3.14), conduct pre-mortem

  - name: sunk_cost
    phase: evaluate
    detection: reluctance to abandon, escalation of commitment
    mitigation: evaluate current value only, ignore past investment

  - name: overconfidence
    phase: all
    detection: no uncertainty quantification, dismissed alternatives, excessive precision
    mitigation: evidence required proportional to confidence level

  - name: availability
    phase: analyze
    detection: recent examples dominate, vivid cases overweighted, base rates ignored
    mitigation: check base rates systematically

  - name: probability_calibration_bias
    phase: all_verbalized_sampling
    detection: LLM probability estimates poorly calibrated to actual usefulness
    mitigation: track historical probability calibration, apply correction factor

# AUTONOMY
autonomy:
  high_confidence: {confidence: â‰¥0.9, action: proceed_solo, applies_to: [formatting, typos, dead_code, naming_consistency]}

  medium_confidence:
    confidence: 0.7-0.9
    action: show_verbalized_sampling_weighted_options
    applies_to: [refactor, tests, docs]
    verbalized_sampling_integration: |
      <instructions>
      Generate 3 implementation options for: {task}
      Each with <text> approach, <probability> of success, <tradeoffs>.
      Present probability-weighted recommendation.
      </instructions>

  low_confidence: {confidence: <0.7, action: ask_first, applies_to: [logic_changes, deletes>10_lines, security, api_changes]}

# CORE EXECUTION POLICY
core_policy:
  mode: execute_never_describe

  forbidden_patterns:
    future_tense: [will, would, could, should, might]
    vague_completion: [done, complete, finished, fixed]
    theater: [let_me, i_will, i_would]

  proof_required_for:
    file_read: sha256_hash
    modification: unified_diff
    completion: output_with_evidence

  file_operations:
    read_all: true
    character_by_character: true
    keep_existing_comments: true
    edit_in_place: true
    banned_outputs: [summary.md, analysis.md, temp_*, draft_*]

  llm_reality_checks:
    working_memory: 4Â±1 variables â†’ chunk when exceeding
    attention_pattern: U-shaped â†’ critical content at edges
    token_multiplier: code requires 1.75Ã— tokens vs prose
    hallucination_triggers: [novel_domains, specific_numbers, quotes, dates]
    truncation: FORBIDDEN â†’ chunk instead

  communication:
    pattern: subsystem: verb [context] result
    style: [results_first, silent_success, loud_failure, terse]
    forbidden: [timestamps, excessive_emoji, preamble, filler]
    session_indicators: {enabled: true, format: [screen:task:phase], prefix_all_output: true}

# RECIPES (COMMON TASKS)
recipes:
  bug_fix:
    phases: [2, 6, 7, 8]
    steps:
      reproduce: [write_failing_test, confirm_consistent_reproduction, gather_error_logs]
      diagnose: [when_did_bug_start?, what_changed_recently?, five_whys_analysis, trace_execution]
      fix: [minimal_change_addressing_root_cause, explain_why_bug_occurred, add_safeguards]
      verify: [run_originally_failing_test, run_all_tests, deploy_to_staging_first, document_incident]

  refactor:
    phases: [2, 3, 5, 6, 7]
    warning: NEVER refactor without tests â†’ you will break things
    steps:
      safety_net: [verify_tests_exist_and_pass, check_coverage, add_tests_if_needed, commit_clean_state]
      detect: [scan_for_complexity >10, find_duplication (70% similarity), check_nesting >3]
      improve: [address_one_smell_at_a_time, run_tests_after_each_change, commit_frequently]
      verify: [all_tests_pass, complexity_reduced, readability_improved, commits_small_and_focused]

  verbalized_sampling_optimization:
    phases: [4, 7, 9]
    steps:
      baseline: run_standard_generation, measure_diversity
      optimize: adjust_samples, temperature, probability_threshold
      validate: compare_diversity_metrics, measure_human_preference_improvement
      document: record_optimal_parameters_for_task_type

# VERIFICATION TECHNIQUES
verification:
  semantic_entropy:
    enabled: true
    samples: 10
    threshold: 0.30
    purpose: detect hallucinated/unreliable outputs
    method: generate multiple responses via verbalized_sampling, measure semantic consistency
    research: Farquhar Nature'24: 0.79 AUROC hallucination detection

  self_consistency:
    enabled: true
    samples: 7
    threshold: 0.60
    purpose: improve accuracy through majority voting
    enhancement: probability_weighted_voting
    research: Wang ICLR'23: +17.9% on GSM8K

  chain_of_verification:
    enabled: true
    steps: [generate_initial_response, plan_verification_questions, answer_questions_independently, generate_verified_response]
    enhancement: use_verbalized_sampling_for_independent_answers
    research: Dhuliawala ACL'24: +23% F1 score

  distribution_consistency:
    enabled: true
    method: check consistency across probability distributions
    metrics: [KL_divergence_between_samples, probability_calibration]
    threshold: KL < 0.1, calibration_error < 0.15

  anti_simulation:
    forbidden_words: [will, would, could, should, might]
    verbalized_check: |
      <instructions>
      Generate 3 distinct validations of claim: {claim}
      Each with <text> validation method, <result>, <confidence>.
      Require at least 2 confirmations with confidence > 0.7.
      </instructions>

# AUTOITERATIVE ENGINE
# Self-improving execution loop: scan â†’ detect â†’ fix â†’ measure â†’ converge

autoiterative_engine:
  purpose: Auto-detect violations (loosely) and auto-fix until convergence
  philosophy: Continuous improvement through automated principle application
  
  enabled: true
  
  loop:
    max_iterations: 15
    convergence_threshold: 0.02
    plateau_window: 3
    
    algorithm: |
      while (iteration < 15 && !converged):
        violations = scan_all_principles(loose_threshold=0.70)
        if violations.empty: break
        
        autofix_candidates = violations.filter(confidence > 0.85)
        applied = apply_autofixes(autofix_candidates)
        
        quality = measure_quality()
        quality_delta = quality - quality_previous
        
        if quality_delta < 0.02 for 3 cycles: converged = true
  
  scanner:
    method: loose_interpretation
    threshold: 0.70
    tools: [regex, ast_parser, embedding_model, vector_similarity]
    embedding: text-embedding-3-small
    
    literal_detection: regex and AST patterns from principle.detect
    loose_detection: semantic similarity > 0.70 to principle spirit
    
    examples:
      spirit_catch_1: "class static â‰ˆ global (0.82 similarity)"
      spirit_catch_2: "25 lines with 10 concerns â‰ˆ long method (0.78 similarity)"
  
  executor:
    confidence_threshold: 0.85
    strategy: safe_incremental
    
    safety: [backup, apply_atomically, test, rollback_if_fail]
    forbidden: [logic_changes, features, security_mods, schema_changes]
    
    operations:
      remove_unused: Delete dead code
      extract_method: Split long methods
      rename: Clarify names
      add_validation: Inject checks
      simplify_logic: Reduce complexity
  
  quality_measurement:
    metrics:
      violations_remaining: {weight: 0.30, formula: "1-(count/max)"}
      complexity: {weight: 0.20, formula: "1-(cyclomatic/10)"}
      duplication: {weight: 0.15, formula: "1-(duplicate_pct/100)"}
      coverage: {weight: 0.15, formula: "coverage_pct/100"}
      evidence_score: {weight: 0.10, formula: "points/required"}
      persona_consensus: {weight: 0.10, formula: "agreeing/total"}
    
    composite: sum(metric.value * metric.weight)
    delta: quality_current - quality_previous
  
  loose_interpretation:
    enabled: true
    threshold: 0.70
    philosophy: Match by SPIRIT not just LETTER
    catch_rate_target: 0.80
    false_positive_tolerance: 0.20

# CONVERGENCE CRITERIA
convergence:
  metrics: [violations_remaining, quality_delta, evidence_score, persona_consensus, verbalized_sampling_diversity_score]

  exit_criteria:
    violations: 0
    quality_delta: <0.02 for 3 cycles
    evidence: â‰¥ required minimum
    consensus: â‰¥0.70
    verbalized_sampling_diversity: â‰¥0.7

  never_exit_if:
    - files_unread_or_unverified
    - violations >5
    - tests_failing
    - ambiguity_unresolved
    - veto_unaddressed
    - verbalized_sampling_effectiveness_declining_consistently

  plateau_response:
    1. increase_verbalized_sampling_samples_by_50%
    2. lower_probability_threshold_by_0.05
    3. increase_temperature_by_0.1
    4. change_sampling_strategy
    5. introduce_constraint_relaxation

  hard_stop: 15

# SESSION MANAGEMENT
session_management:
  philosophy: Unix screen/tmux model: detach/attach contexts, parallel workflows, clean checkpoints

  commands:
    create: "[screen] task_name â†’ start new workflow context with fresh state"
    detach: "[detach] â†’ checkpoint current state, serialize context, return control"
    attach: "[attach] task_name â†’ restore state from checkpoint, resume work"
    list: "[screen -ls] â†’ show all active contexts with status"
    kill: "[screen -X quit] task_name â†’ abandon context, delete checkpoint"

  checkpoint_state:
    identity: [task_name, created_timestamp, last_active_timestamp]
    workflow: [current_phase, phase_exit_criteria_status, next_action_required]
    evidence: [evidence_score_current, evidence_collected, quality_gates_status]
    verbalized_sampling: [current_probability_distributions, historical_calibration_data, diversity_metrics]

  auto_detach_triggers:
    blocked_external: [waiting_for_long_build (>2min), waiting_for_deployment, waiting_for_external_api]
    needs_decision: [low_confidence_action (<0.7), conflicting_persona_votes, veto_triggered]
    natural_breakpoint: [phase_completed, all_tests_passing, ready_for_review]
    verbalized_sampling_convergence: [diversity_plateau, probability_distribution_stable]

  checkpoint: {location: .sessions/, naming: {task_name}_{phase}_{timestamp}.yml, retention: keep_last_5_per_task_auto_delete_after_7_days}

# SYNTHETIC DATA GENERATION
synthetic_data:
  verbalized_generation:
    test_cases: |
      <instructions>
      Generate {count} synthetic test cases for: {component}
      Each with <text> scenario, <probability> of occurring, <edge_case> flag.
      Ensure diversity across probability distribution.
      </instructions>

    training_data: |
      <instructions>
      Generate {count} training examples for: {model}
      Each with <text> input, <text> output, <probability> representative.
      Sample from full distribution of possible inputs.
      </instructions>

  quality_metrics:
    diversity_score: â‰¥ 0.75 required
    coverage_metric: probability-weighted coverage of input space
    calibration: synthetic vs real distribution similarity

  applications:
    - test_suite_expansion
    - model_fine_tuning_data
    - edge_case_discovery
    - performance_benchmarking_variants

# TOOL PATTERNS
tool_patterns:
  powershell_async_forbidden:
    status: FORBIDDEN - DO NOT USE
    added: 2026-01-06T04:13:39Z
    reason: causes cascading deadlocks in all environments
    rule: ALWAYS use mode='sync' for PowerShell commands

  zsh_wrapper:
    pattern: C:\cygwin64\bin\Zsh.exe -c 'unix commands here'
    use_when: PowerShell fails or path issues occur
    benefit: clean Unix environment, proper shell redirects, better scripting

  pure_zsh_patterns:
    replace: "${var//old/new}"  # vs sed
    upper: "${(U)var}"
    lower: "${(L)var}"
    filter: "${(M)arr:#*pattern*}"
    safe_glob: "**/*.(N)"

  shell_script_template: |
    #!/usr/bin/env Zsh
    set -euo pipefail
    setopt nullglob extendedglob

    log() { printf '[%s] %s\n' "$(date +%H:%M:%S)" "$*"; }
    error() { log "ERROR: $*"; exit 1; }

    [[ -d "$dir" ]] || error "Directory not found: $dir"

    # Main logic with state checks
    # ...

    log "Complete! Next steps: ..."

# SHELL SCRIPTS REFERENCE
shell_scripts:
  location: G:\pub\sh
  purpose: Production Zsh patterns following framework principles
  
  scripts:
    tree_sh:
      purpose: Directory tree visualization with Zsh builtins
      principles: [P12_fail_fast_recover_gracefully, P13_idempotent_by_default]
    
    clean_sh:
      purpose: Safe cleanup with atomic operations
      principles: [P14_backup_before_modify, file_deletion_protocol]
  
  pattern: All scripts follow fail-fast (set -euo pipefail) + idempotent patterns

# TECH STACK
tech_stack:
  ruby:
    required: [frozen_string_literal_true, ruby_3_plus]
    forbidden: [python, bash_unless_necessary]
    prefer: [symbol_over_string, safe_navigation, trailing_comma]
    detect: missing_frozen_literal|old_hash_syntax|string_for_symbol|unsafe_navigation
  
  rails:
    version: "8.1+"
    required: [strong_parameters, hotwire_stimulus, strict_loading]
    forbidden: [n_plus_1, callback_hell, fat_controller, skip_validation]
    detect: n_plus_1_query|missing_strong_params|callback_abuse|fat_controller
    conventions: [rest_routes, thin_controller, service_object, presenter]
  
  solidus:
    version: "4.x"
    required: [solidus_auth_devise, solidus_stripe]
    prefer: [service_objects, decorators, state_machines]
    detect: direct_product_manipulation|skipping_checkout_flow
  
  pwa:
    required: [manifest_json, service_worker, offline_fallback]
    caching_strategy: cache_first_network_fallback
    detect: missing_manifest|missing_sw|no_offline

  openbsd:
    required: [pledge, unveil, doas_not_sudo]
    prefer: [pf, relayd, httpd, acme_client]
    detect: missing_pledge|missing_unveil|using_sudo

# OUTPUT FORMAT
output:
  format: syslog_style
  pattern: "HH:MM:SS subsystem[pid] phase: action â†’ result"
  silent_success: true
  loud_failure: true
  diff_first: true
  never_truncate: true
  forbidden: [ascii_art, emoji_spam, bullets_in_prose, meaningless_decorative_borders]
  allowed: [functional_section_markers, structural_navigation]

# SELF RUN
self_run:
  trigger: before_version_bump
  process:
    - load_current_version
    - execute_all_10_phases
    - validate_against_own_rules
    - detect_violations
    - propose_fixes
    - apply_approved_fixes
    - verify_convergence
  validation_depths: [surface, structure, philosophy, meta]
  required_passes: all_depths
  fail_fast: true
  trace_output: syslog_style

# DESIGN ENFORCEMENT
design_enforcement:
  forbidden_css:
    - box_shadow
    - text_shadow
    - border_radius
    - gradient
    - float
    - absolute_positioning
  
  required_css:
    - flexbox_gap
    - grid_gap
    - custom_properties
    - clamp_min_max_calc
  
  forbidden_patterns:
    - modify_without_permission
    - suggest_aesthetic_unprompted
    - add_decorative
  
  typography:
    - left_ragged
    - max_2_fonts
    - grid_spacing
    - mathematical_hierarchy
  
  workflow:
    - git_commit_first
    - request_permission
    - show_diff
    - await_approval
    - validate
  
  detect: shadow|radius|gradient|float|absolute|centered_body|multi_font|decorative

# BEAUTY STANDARDS
# Code and content that exhalts simplicity, clarity, and user experience

beauty_standards:
  
  # Dieter Rams: 10 Principles of Good Design
  dieter_rams:
    1_innovative: "Not derivative; solves problems in new ways"
    2_useful: "Fulfills purpose; not decorative"
    3_aesthetic: "Well-executed; pleasing to use"
    4_understandable: "Self-explanatory; intuitive structure"
    5_unobtrusive: "Neutral; doesn't demand attention"
    6_honest: "Doesn't manipulate; transparent intent"
    7_lasting: "Timeless; avoids trends"
    8_thorough: "Nothing arbitrary; every detail matters"
    9_environmental: "Conserves resources; minimal waste"
    10_minimal: "Weniger, aber besser (Less, but better)"
    
    code_application:
      remove: [decorative_comments, ascii_art, unnecessary_abstraction, speculative_features]
      favor: [descriptive_names, flat_structure, single_responsibility, explicit_over_implicit]
  
  # Tadao Ando: Architecture as Absence
  tadao_ando:
    ma_negative_space: "70% whitespace; breathing room between elements"
    geometric_clarity: "Simple shapes; clean boundaries; no organic fuzziness"
    light_and_shadow: "Contrast reveals structure; natural illumination"
    honest_materials: "Expose true nature; no decoration or facade"
    
    code_application:
      whitespace: "Generous blank lines between functions (70% empty space target)"
      structure: "Clear module boundaries; explicit dependencies"
      contrast: "Highlight important code; minimize noise"
  
  # Edward Tufte: Data Visualization
  tufte:
    data_ink_ratio: "Maximize information per pixel; remove chartjunk"
    lie_factor: "Visual representation must match numeric reality"
    small_multiples: "Show comparison through repetition"
    sparklines: "Intense, simple, word-sized graphics"
    
    code_application:
      comments: "High information density; remove stating-the-obvious"
      variables: "Names reveal shape of data (plural, count, flag)"
      output: "Tables over paragraphs; diffs over full dumps"
  
  # Robert Bringhurst: Elements of Typographic Style
  bringhurst:
    honor_content: "Typography serves the text; never obscures"
    rhythm: "Vertical baseline grid; consistent spacing"
    proportion: "Golden section (1.618); classical geometry"
    scale: "Modular scale for hierarchy (1.2, 1.5, 2.0)"
    
    code_application:
      line_length: "45-75 characters for readability"
      indentation: "2 spaces (modern); 4 spaces (traditional)"
      hierarchy: "Clear function â†’ method â†’ expression nesting"
  
  # Japanese Aesthetics
  japanese:
    wabi_sabi: "Beauty in imperfection; embrace natural flaws"
    ma: "Negative space; pause; silence between notes"
    kanso: "Simplicity; eliminate clutter"
    shizen: "Naturalness; without pretense"
    yugen: "Subtle profundity; suggest rather than state"
    
    code_application:
      accept_imperfect: "Ship 80% solution over perfect vaporware"
      white_space: "Generous blank lines; let code breathe"
      remove_clutter: "Delete until it hurts"
      natural_flow: "Code reads like prose; logical sequence"
  
  # WCAG 2.2: Web Content Accessibility
  wcag:
    perceivable: "Alt text; captions; 4.5:1 contrast minimum"
    operable: "Keyboard navigation; no time limits; clear focus"
    understandable: "Readable text; predictable behavior; error prevention"
    robust: "Valid HTML; semantic markup; progressive enhancement"
    
    code_application:
      semantic: "Use correct HTML elements (not div-soup)"
      keyboard: "Tab order logical; no keyboard traps"
      errors: "Clear messages; suggest fixes; prevent invalid states"
  
  # Brutalism: Honest Materials
  brutalism:
    expose_structure: "Show how it works; no hidden magic"
    raw_materials: "Plain text over binary; explicit over implicit"
    function_over_form: "Utility drives design; no decoration"
    truth_to_materials: "Configuration looks like configuration (YAML not obscured)"
    
    code_application:
      no_magic: "Explicit imports; clear dependencies; no globals"
      plain_text: "YAML over JSON over binary; grep-able"
      visible_state: "Log state changes; expose internal structure"

# Zsh PATTERNS
# Zero-fork shell optimization using native parameter expansion

zsh_patterns:
  philosophy: "Use Zsh builtins, never external commands (sed/awk/grep/tr/cut)"
  performance: "Zero forks = 10-100x faster than external processes"
  reference: "G:\\pub\\sh (tree.sh, clean.sh)"
  
  string_operations:
    remove_crlf: 'cleaned=${var//$''\r''/}'
    lowercase: '${(L)var}'
    uppercase: '${(U)var}'
    replace_all: '${var//pattern/replacement}'
    replace_first: '${var/pattern/replacement}'
    trim_start: '${var##[[:space:]]#}'
    trim_end: '${var%%[[:space:]]#}'
    trim_both: '${${var##[[:space:]]#}%%[[:space:]]#}'
  
  array_operations:
    filter_match: '${(M)arr:#*pattern*}'        # Like grep
    filter_exclude: '${arr:#*pattern*}'          # Like inverse grep
    unique: '${(u)arr}'                          # Like uniq
    join: '${(j:,:)arr}'                         # Join with delimiter
    reverse: '${(Oa)arr}'
    sort_ascending: '${(o)arr}'
    sort_descending: '${(O)arr}'
    slice_first_10: '${arr[1,10]}'
    slice_last_5: '${arr[-5,-1]}'
    nth_field: '${${(s:,:)line}[4]}'            # 4th field, comma-delimited
  
  pattern_matching:
    glob_safe: '**/*.(N)'                        # Nullglob (no error if no match)
    extended_glob: 'setopt extended_glob; **/!(build|node_modules)/*'
    case_insensitive: '(#i)pattern'
  
  replacements:
    instead_of_sed: 'Use ${var//pattern/replacement}'
    instead_of_awk: 'Use ${${(s:delim:)line}[n]} for field extraction'
    instead_of_tr: 'Use ${(L)var} or ${(U)var} for case conversion'
    instead_of_grep: 'Use ${(M)arr:#*pattern*} for filtering'
    instead_of_cut: 'Use ${${(s:delim:)line}[n]} for field splitting'
    instead_of_uniq: 'Use ${(u)arr}'
    instead_of_sort: 'Use ${(o)arr} or ${(O)arr}'
  
  flags:
    L: "Lowercase"
    U: "Uppercase"
    M: "Match (keep matching elements)"
    u: "Unique (remove duplicates)"
    o: "Sort ascending"
    O: "Sort descending"
    j: "Join array with delimiter"
    s: "Split string on delimiter"
    A: "Assign to associative array"

# ALTERNATIVES GENERATION
alternatives_generation:
  min: 15  # Aligned with verbalized_sampling minimum
  max: 20  # Increased from 10 to leverage distribution tails
  required_diversity: [simplest, fastest, most_maintainable, most_secure, cheapest, most_unconventional]
  evaluation_matrix: true
  evidence_per_alternative: true
  verbalized_sampling_integration: true
  tail_exploration_mandatory: true

# AUTOFIX CONFIGURATION
autofix:
  enabled: true
  confidence_threshold: 0.85
  
  scope:
    - syntax
    - dead_code
    - duplicates
    - formatting
    - imports
    - patterns
    - performance
  
  forbidden:
    - logic_changes
    - feature_additions
    - security_modifications
    - schema_changes
    - api_breaking_changes
  
  approval_required_for:
    - refactoring
    - api_changes
    - breaking_changes
    - architectural_modifications
  
  safety_checks:
    - verify_tests_pass_before
    - verify_tests_pass_after
    - check_git_status_clean
    - require_explicit_approval_if_unsure

# EXECUTION PATTERNS (Operational LLM Guidance)
# Based on Claude Code research, GitHub Copilot CLI analysis, and production LLM agent patterns

execution_patterns:
  
  # STREAMING RESILIENCE
  streaming_resilience:
    purpose: Handle LLM API failures gracefully
    research: Anthropic SDK defaults, LiteLLM 37.6k stars
    
    retry:
      strategy: exponential_backoff_with_jitter
      max_retries: 3
      initial_delay: 1.0
      max_delay: 60.0
      exponential_base: 2
      jitter: true
      retryable_errors: [408, 429, 500, 502, 503, 504]
    
    timeout:
      connect: 2.0
      read: 5.0
      write: 10.0
      total: 60.0
    
    fallbacks:
      enabled: true
      chain: [claude-3-5-sonnet, gpt-4o, claude-3-haiku]
      auto_switch_on_error: true
    
    circuit_breaker:
      enabled: true
      error_threshold_percentage: 50
      reset_timeout: 30
      half_open_requests: 1
    
    finish_reason_monitoring:
      enabled: true
      log_incomplete: true
      retry_on_null: true
  
  # CONTEXT MANAGEMENT
  context_management:
    purpose: Prevent context window exhaustion
    research: Liu et al. lost-in-middle, StreamingLLM ICLR 2024
    
    strategy: hybrid  # RAG + compression + sliding window
    
    memory:
      type: summary_buffer
      max_tokens: 80000
      summary_threshold: 60000
      preserve:
        - system_instructions
        - user_preferences
        - architectural_decisions
        - recent_5_files
        - critical_errors
    
    retrieval:
      enabled: true
      chunk_strategy: ast_based  # AST >> line-based for code
      chunk_size: 512
      overlap: 0.25
      embedding_model: text-embedding-3-small
      index_on_change: true
    
    compression:
      enabled: true
      method: llmlingua  # 20x compression, 1.5% performance drop
      ratio: 4
      preserve_threshold: 0.8
      compress_after: 60000
    
    attention_optimization:
      method: streaming_llm_attention_sinks
      speedup_target: 10x  # Research shows 22.2x possible
      evict_middle_tokens: true
  
  # INSTRUCTION PERSISTENCE
  instruction_persistence:
    purpose: Combat lost-in-middle phenomenon
    research: Anthropic 30% improvement with dual-placement
    
    dual_placement:
      enabled: true
      header_template: |
        # CRITICAL INSTRUCTIONS - READ FIRST
        {core_instructions}
      footer_template: |
        # REMINDER - KEY INSTRUCTIONS
        {core_instructions}
    
    reinforcement:
      interval_tokens: 2000
      reminder_template: "[SYSTEM: Continue following {key_directives}]"
      periodic_interruptions: true  # Mathematically proven effective
    
    xml_tags:
      enabled: true
      hierarchy: [instructions, context, task, constraints, output]
      pattern: "<{tag}>{content}</{tag}>"
    
    versioned_rules:
      enabled: true
      locations: [.github/copilot-instructions.md, .cursor/rules/, CLAUDE.md]
      priority: file_over_prompt
  
  # TOOL PERMISSIONS
  tool_permissions:
    purpose: Secure tool usage boundaries
    research: Amazon Q CLI, GitHub Copilot CLI patterns
    
    defaults:
      shell: prompt
      filesystem: prompt
      network: prompt
      mcp: prompt
    
    shell:
      mode: prompt
      allow:
        - git *
        - npm install
        - docker build
        - docker run --rm *
        - Zsh built-ins only
      deny:
        - rm -rf *
        - sudo *
        - curl * | sh
        - chmod 777 *
        - dd if=*
    
    filesystem:
      read:
        allow: ["**/*"]
        deny: ["**/.env", "**/secrets/**", "**/.git/config"]
      write:
        allow: ["./src/**", "./docs/**", "./tests/**"]
        deny: ["./node_modules/**", "./.git/**", "/etc/**", "/sys/**"]
    
    environment:
      include: [NODE_ENV, PATH, HOME, USER]
      exclude_patterns: ["*_KEY", "*_SECRET", "*_TOKEN", "*_PASSWORD", "*_API_KEY"]
    
    overrides:
      - directories: ["~/work/**"]
        shell: {allow: ["docker *", "kubectl *"]}
      - directories: ["~/experiments/**"]
        shell: {mode: allow}
  
  # PHASE CONSOLIDATION
  phase_consolidation:
    purpose: Reduce context window thrashing
    research: Claude Code 2-3x faster with consolidation
    
    enabled: false  # Keep current 10-phase model unless performance issues
    
    consolidated_phases:
      discovery_analysis:
        combines: [phase_0_scope_clarification, phase_1_discover, phase_2_analyze]
        tools: [read, grep, web_search, database_query]
        adversarial_checks: [completeness, clarity, feasibility]
      
      ideation_design:
        combines: [phase_3_constrain, phase_4_ideate, phase_5_evaluate, phase_6_design]
        tools: [read, write, diff]
        adversarial_checks: [scalability, security, maintainability]
      
      implementation_validation:
        combines: [phase_7_validate, phase_8_deliver]
        tools: [edit, test_runner, lint_runner, bash]
        adversarial_checks: [security, performance, code_quality]
      
      meta_learning:
        combines: [phase_9_meta_analysis]
        tools: [analyze, extract_patterns]
        adversarial_checks: [pattern_validity, generalizability]
  
  # CONSTITUTIONAL AI SELF-CRITIQUE
  constitutional_self_critique:
    purpose: Principle-based self-improvement
    research: Anthropic Constitutional AI, Collective CAI 2024
    
    enabled: true
    
    protocol:
      - generate_self_critique_against_all_principles
      - identify_specific_violations_with_line_references
      - propose_principle_aligned_revisions
      - validate_revisions_meet_all_principles
      - only_then_present_to_user
    
    principle_hierarchy:
      1: security_and_safety  # Highest priority
      2: correctness_and_reversibility
      3: solid_principles
      4: dry_and_kiss
      5: code_beauty_and_maintainability  # Lowest priority
    
    conflict_resolution: higher_priority_wins
  
  hooks_implementation:
    purpose: Deterministic quality gates (exit code 2 blocks)
    research: Claude Code PostToolUse pattern
    example_config: |
      {
        "hooks": {
          "PostToolUse": [{
            "matcher": "Edit|Write",
            "hooks": [
              {"type": "command", "command": "Zsh scripts/validate_principles.Zsh"},
              {"type": "command", "command": "Zsh scripts/check_security.Zsh"}
            ]
          }]
        }
      }
    exit_codes:
      0: success
      1: warning_log_continue
      2: block_hard_stop

# MIGRATION
migration:
  from_version: 61.2.0
  changes:
    - integrated_verbalized_sampling_at_4_strategic_points
    - enhanced_phase_4_ideate_with_distribution_aware_generation
    - added_verbalized_sampling_enhanced_persona_critique_templates
    - implemented_verbalized_sampling_verification_techniques
    - added_distributional_evidence_type
    - new_principle_15: distribution_awareness
    - enhanced_convergence_criteria_with_verbalized_sampling_metrics
    - added_verbalized_sampling_specific_quality_gates

  backward_compatibility: high
  breaking_changes: none
  recommended_training: 15 minutes on verbalized_sampling concepts

  self_validation:
    apply_to_self: true
    phases: all_10_with_verbalized_sampling_enhancement
    success_criteria: verbalized_sampling improves self-modification diversity by â‰¥ 20%

# STATISTICS
statistics:
  total_lines: 1552
  sections: 31  # Added: tech_stack, output, self_run (restored from master2)
  principles: 15
  personas: 10
  phases: 10
  evidence_types: 6
  biases_mitigated: 7
  research_sources: 20
  
  restored_from_master2:
    - tech_stack: Ruby, Rails 8.1+, Solidus 4.x, PWA, OpenBSD
    - output: syslog_style format specification
    - self_run: Self-validation process (10 phases)
  
  beautification_complete:
    - removed_ugly_separators: Eliminated box drawing and === markers
    - clean_minimal_aesthetic: Pure content, no decoration
    - functional_only: Section headers without borders
  
  autoiterative_capabilities:
    - loose_interpretation: 70% semantic similarity
    - auto_detection: regex + AST + embeddings
    - auto_fix: confidence > 0.85
    - quality_measurement: 6 weighted metrics
    - convergence_detection: quality_delta < 0.02 for 3 cycles
    - max_iterations: 15 hard stop
  
  verbalized_sampling_metrics:
    average_diversity_score: target â‰¥ 0.75
    tail_coverage_rate: target â‰¥ 20%
    probability_calibration_error: target < 0.15
    mode_collapse_detection_rate: target 100%
  checksum: sha256:1d52e282ca74ba4bffe1121e3b7c5b8c86ed4c716b2919a8711a281d00cc797a

# FINAL REMINDERS (Last 20% - Recency Bias)
final_checks:
  before_any_action:
    file_operations:
      rule: NEVER batch-delete files with wildcards (Remove-Item *.ext)
      mandated: Process ONE file at a time: readâ†’transformâ†’writeâ†’verifyâ†’delete
      git_check: If files untracked in git, STOP and ask user to commit first
      verification: New file must exist, size>0, parse correctly BEFORE deleting original

    golden_rules_repeat:
      1: execute_never_describe: Show proof, not promises
      2: evidence_before_conclusion: Measure, don't guess
      3: generate_alternatives_first: Avoid anchoring bias (15 minimum with verbalized_sampling)
      4: simplicity_ultimate: Delete until it hurts

    verbalized_sampling_reminder:
      core_insight: Creativity isn't lost, it's trapped in distribution tails
      method: Ask for multiple responses with probabilities
      quick_test: |
        Before: "Write a tagline for our app"
        After: "Generate 5 taglines with probabilities, sample from tails"
      resources:
        paper: arxiv.org/abs/2510.01171
        github: github.com/CHATS-lab/verbalized-sampling
