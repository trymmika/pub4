# frozen_string_literal: true
require 'ferrum'

require 'nokogiri'

require 'fileutils'
require 'uri'
require 'digest'
# Universal Scraper with Ferrum for web content and screenshots
# Includes cognitive load awareness and depth-based analysis

class UniversalScraper
  attr_reader :browser, :config, :cognitive_monitor
  def initialize(config = {})
    @config = default_config.merge(config)

    @cognitive_monitor = nil
    @screenshot_dir = @config[:screenshot_dir]
    @max_depth = @config[:max_depth]
    @timeout = @config[:timeout]
    @user_agent = @config[:user_agent]
    # Ensure screenshot directory exists
    FileUtils.mkdir_p(@screenshot_dir)

    # Initialize browser with error handling
    initialize_browser

  end
  # Set cognitive monitor for load-aware processing
  def set_cognitive_monitor(monitor)

    @cognitive_monitor = monitor
  end
  # Main scraping method with cognitive awareness
  def scrape(url, options = {})

    # Check cognitive capacity
    if @cognitive_monitor&.cognitive_overload?
      puts 'üß† Cognitive overload detected, deferring scraping'
      return { error: 'Cognitive overload - scraping deferred' }
    end
    # Validate URL
    return { error: 'Invalid URL' } unless valid_url?(url)

    begin
      puts "üï∑Ô∏è Scraping #{url}..."

      # Navigate to page
      @browser.go_to(url)

      wait_for_page_load
      # Take screenshot
      screenshot_path = take_screenshot(url)

      # Extract content
      content = extract_content

      # Analyze page structure
      analysis = analyze_page_structure

      # Extract links for depth-based scraping
      links = extract_links(url) if options[:extract_links]

      # Update cognitive load
      if @cognitive_monitor

        complexity = calculate_content_complexity(content)
        @cognitive_monitor.add_concept(url, complexity * 0.1)
      end
      result = {
        url: url,

        title: content[:title],
        content: content[:text],
        html: content[:html],
        screenshot: screenshot_path,
        analysis: analysis,
        links: links,
        timestamp: Time.now,
        success: true
      }
      puts "‚úÖ Successfully scraped #{url}"
      result

    rescue StandardError => e
      puts "‚ùå Scraping failed for #{url}: #{e.message}"
      { url: url, error: e.message, success: false }
    end
  end
  # Scrape multiple URLs with cognitive load balancing
  def scrape_multiple(urls, options = {})

    results = []
    urls.each_with_index do |url, index|
      # Check cognitive state before each scrape

      if @cognitive_monitor&.cognitive_overload?
        puts "üß† Cognitive overload detected, stopping batch scrape at #{index}/#{urls.size}"
        break
      end
      result = scrape(url, options)
      results << result

      # Brief pause between requests
      sleep(1) if options[:delay]

      # Progress update
      puts "üìä Progress: #{index + 1}/#{urls.size} URLs scraped"

    end
    results
  end

  # Deep scrape with configurable depth
  def deep_scrape(start_url, depth = nil, visited = Set.new)

    depth ||= @max_depth
    return [] if depth <= 0 || visited.include?(start_url)
    # Check cognitive capacity
    if @cognitive_monitor&.cognitive_overload?

      puts 'üß† Cognitive overload detected, stopping deep scrape'
      return []
    end
    visited.add(start_url)
    results = []

    # Scrape current page
    result = scrape(start_url, extract_links: true)

    results << result if result[:success]
    # Recursively scrape linked pages
    if result[:success] && result[:links]

      result[:links].take(5).each do |link| # Limit to 5 links per page
        next if visited.include?(link) || !same_domain?(start_url, link)
        deeper_results = deep_scrape(link, depth - 1, visited)
        results.concat(deeper_results)

      end
    end
    results
  end

  # Extract content from current page
  def extract_content

    title = @browser.evaluate('document.title') || ''
    # Extract main text content
    text_content = @browser.evaluate(<<~JS)

      // Remove script and style elements
      var scripts = document.querySelectorAll('script, style, nav, footer, aside');
      scripts.forEach(function(el) { el.remove(); });
      // Get main content areas
      var main = document.querySelector('main, article, .content, #content, .post, .article');

      if (main) {
        return main.innerText;
      }
      // Fallback to body content
      return document.body.innerText;

    JS
    # Get full HTML
    html = @browser.evaluate('document.documentElement.outerHTML')

    {
      title: title.strip,

      text: clean_text(text_content || ''),
      html: html
    }
  end
  # Take screenshot of current page
  def take_screenshot(url)

    # Generate filename based on URL
    filename = generate_screenshot_filename(url)
    filepath = File.join(@screenshot_dir, filename)
    # Take screenshot
    @browser.screenshot(path: filepath, format: 'png', quality: 80)

    puts "üì∏ Screenshot saved: #{filepath}"
    filepath

  rescue StandardError => e
    puts "‚ùå Screenshot failed: #{e.message}"
    nil
  end
  # Analyze page structure and content
  def analyze_page_structure

    structure = @browser.evaluate(<<~JS)
      function analyzeStructure() {
        var analysis = {
          headings: [],
          forms: [],
          images: [],
          links: 0,
          interactive_elements: 0,
          content_sections: 0
        };
      #{'  '}
        // Analyze headings
        var headings = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
        headings.forEach(function(h) {
          analysis.headings.push({
            level: h.tagName,
            text: h.innerText.substring(0, 100)
          });
        });
      #{'  '}
        // Analyze forms
        var forms = document.querySelectorAll('form');
        forms.forEach(function(form) {
          var inputs = form.querySelectorAll('input, select, textarea').length;
          analysis.forms.push({
            action: form.action || '',
            method: form.method || 'GET',
            inputs: inputs
          });
        });
      #{'  '}
        // Count elements
        analysis.images = document.querySelectorAll('img').length;
        analysis.links = document.querySelectorAll('a[href]').length;
        analysis.interactive_elements = document.querySelectorAll('button, input, select, textarea').length;
        analysis.content_sections = document.querySelectorAll('article, section, .content, .post').length;
      #{'  '}
        return analysis;
      }
      analyzeStructure();
    JS

    structure || {}
  end

  # Extract links from current page
  def extract_links(base_url)

    links = @browser.evaluate(<<~JS)
      var links = [];
      var anchors = document.querySelectorAll('a[href]');
      anchors.forEach(function(a) {
        var href = a.href;

        if (href && !href.startsWith('javascript:') && !href.startsWith('mailto:')) {
          links.push(href);
        }
      });
      return links;
    JS

    # Convert relative URLs to absolute
    (links || []).map do |link|

      resolve_url(base_url, link)
    end.compact.uniq
  end
  # Close browser
  def close

    @browser&.quit
    puts 'üîå Browser closed'
  end
  private
  # Default configuration

  def default_config

    {
      screenshot_dir: 'data/screenshots',
      max_depth: 2,
      timeout: 30,
      user_agent: 'AI3-UniversalScraper/1.0',
      window_size: [1920, 1080],
      headless: true
    }
  end
  # Initialize Ferrum browser
  def initialize_browser

    options = {
      headless: @config[:headless],
      timeout: @config[:timeout],
      window_size: @config[:window_size],
      browser_options: {
        'user-agent' => @user_agent,
        'disable-gpu' => nil,
        'no-sandbox' => nil,
        'disable-dev-shm-usage' => nil
      }
    }
    @browser = Ferrum::Browser.new(**options)
    puts 'üåê Browser initialized'

  rescue StandardError => e
    puts "‚ùå Failed to initialize browser: #{e.message}"
    puts 'üí° Make sure Chrome/Chromium is installed'
    raise
  end
  # Wait for page to load
  def wait_for_page_load(timeout = 10)

    @browser.evaluate_async(<<~JS, timeout)
      if (document.readyState === 'complete') {
        arguments[0]();
      } else {
        window.addEventListener('load', arguments[0]);
      }
    JS
  rescue Ferrum::TimeoutError
    puts '‚ö†Ô∏è Page load timeout'
  end
  # Validate URL format
  def valid_url?(url)

    uri = URI.parse(url)
    uri.is_a?(URI::HTTP) || uri.is_a?(URI::HTTPS)
  rescue URI::InvalidURIError
    false
  end
  # Generate screenshot filename
  def generate_screenshot_filename(url)

    # Create a safe filename from URL
    safe_name = url.gsub(/[^a-zA-Z0-9]/, '_')
    hash = Digest::SHA256.hexdigest(url)[0..8]
    timestamp = Time.now.strftime('%Y%m%d_%H%M%S')
    "#{timestamp}_#{hash}_#{safe_name[0..50]}.png"
  end

  # Clean extracted text
  def clean_text(text)

    # Remove extra whitespace and normalize
    text.gsub(/\s+/, ' ')
        .gsub(/\n\s*\n/, "\n")
        .strip
  end
  # Calculate content complexity for cognitive load
  def calculate_content_complexity(content)

    return 1.0 unless content.is_a?(Hash)
    complexity = 0
    # Text length factor

    text_length = content[:text]&.length || 0

    complexity += (text_length / 1000.0).clamp(0, 3)
    # HTML complexity
    html = content[:html] || ''

    complexity += (html.scan(/<[^>]+>/).size / 100.0).clamp(0, 2)
    # Title complexity
    title = content[:title] || ''

    complexity += (title.split.size / 10.0).clamp(0, 1)
    complexity.clamp(1.0, 5.0)
  end

  # Resolve relative URLs
  def resolve_url(base_url, link)

    URI.join(base_url, link).to_s
  rescue URI::InvalidURIError
    nil
  end
  # Check if URLs are from same domain
  def same_domain?(url1, url2)

    URI.parse(url1).host == URI.parse(url2).host
  rescue URI::InvalidURIError
    false
  end
end
