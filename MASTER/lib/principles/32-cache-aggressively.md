# Cache Aggressively

> Cache LLM responses. Same prompt = same result.

tier: llm
priority: 32
smells: [redundant_api_calls, wasted_tokens]
auto_fixable: true
